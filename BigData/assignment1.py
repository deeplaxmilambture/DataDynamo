# -*- coding: utf-8 -*-
"""Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YvcBqzLhWliJ3sRY9vrPkpQBivIQrCsf
"""

!pip install -q pyspark
!wget -q 'https://drive.google.com/uc?export=download&id=1QkrHJxCU7tonEjrQClTn-30vE93Q8fGh' -O 'Sample.txt'

from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.window import Window


conf = SparkConf().setAppName('SparkWordCount')
sc = SparkContext.getOrCreate(conf = conf)

sqlContext = SparkSession.builder\
        .master("local")\
        .appName("Colab")\
        .config('spark.ui.port', '4050')\
        .getOrCreate()

input_file = sc.textFile('Sample.txt')

#Using sliding window algorithm
window_size = 6

def sliding_window(iterable,window_size):
  windows = []                               #list to store the generated window
  for i in range(len(iterable) - window_size +  1):
    window=[]                               #list to store the current window
    for j in range(window_size):
      window.append(iterable[i+j])
    windows.append(window)
  return windows


try:
  sliding_rdd = input_file.flatMap(lambda line: sliding_window(line.split(),window_size))
  keyPairs = sliding_rdd.map(lambda window: (' '.join(window), 1))
  countKeys = keyPairs.reduceByKey(lambda a, b: a + b)
  countsDF = sqlContext.createDataFrame(countKeys).withColumnRenamed('_1','Word').withColumnRenamed('_2','Count')
  countsDF.toPandas().to_csv('counts.csv', index=False)
  countsDF.show()
except Exception as e:
  print("AN ERROR OCCURED: ",e)
finally:
  sc.stop()